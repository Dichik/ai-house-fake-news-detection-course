{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework 2\n",
    "\n",
    "### Description:\n",
    "- parse https://rusdisinfo.voxukraine.org/\n",
    "- write function to parse news pages\n",
    "- wrap it in the flask app, so when you pass the URL, you receive list of news back\n",
    "- wrap it in the docker container"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1. Imports libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2. Website analysis and variables declaration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this assignment we are going to parse data from **'https://rusdisinfo.voxukraine.org'**. [2]\n",
    "\n",
    "![](images/main-page.png)\n",
    "\n",
    "One thing to note here that we are going to subgroup after pressing one of these hashtags. So we are going to parse all of them by receiving their suffixes to form request url. Link to every group looks like `<url>/narratives/<hashtag>`.\n",
    "\n",
    "Also, we should take into account that this website is dynamic.\n",
    "\n",
    "**Dynamic websites** produce some results based on some action of a user. For example, when a webpage is completely loaded only\n",
    "on scroll down or move the mouse over the screen there must be some dynamic programming behind this. [1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "url = 'https://rusdisinfo.voxukraine.org'\n",
    "narratives = '/narratives'\n",
    "href = 'href'\n",
    "\n",
    "sub_headers = []\n",
    "h2s = []\n",
    "\n",
    "fake_news = set()\n",
    "true_news = set()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we just check if we are going to add a valid subhead's link to our list."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def is_valid_subheader(link):\n",
    "    return link.has_attr(href) \\\n",
    "           and link[href].startswith(narratives) \\\n",
    "           and len(link[href]) != len(narratives)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now it is time to collect all subheaders. We use `BeautifulSoup` for now, because we need just parse html and there is no dynamic content. [5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # parse the HTML content of the page and get <a>...</a>\n",
    "    soup = BeautifulSoup(response.content, 'html.parser', parse_only=SoupStrainer('a'))\n",
    "\n",
    "    for link in soup:\n",
    "        if is_valid_subheader(link):\n",
    "            sub_headers.append((link['href'], link.text[2:]))\n",
    "else:\n",
    "    raise Exception('Failed to fetch the page')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is sub-headers we are going to parse: \n",
      "/narratives/Maidan%20in%202014%20was%20a%20coup%20d'%C3%A9tat\n",
      "/narratives/Nazis%20and%20ultra-nationalists%20in%20Ukraine\n",
      "/narratives/Nord%20Stream-2\n",
      "/narratives/Russia%20needs%20to%20protect%20itself\n",
      "/narratives/Ukraine%20is%20an%20illiberal%20state\n",
      "/narratives/Ukraine%20is%20a%20failed%20state\n",
      "/narratives/The%20West%20controls%20Ukraine%20and%20uses%20it%20to%20its%20advantage\n",
      "/narratives/The%20West%20is%20not%20interested%20in%20dealing%20with%20Ukraine,%20moreover,%20solving%20its%20problems.\n",
      "/narratives/Russians%20are%20discriminated%20in%20Ukraine\n",
      "/narratives/Russia%20is%20not%20an%20agressor%20towards%20Ukraine\n",
      "/narratives/Ukraine%20is%20conducting%20an%20aggressive%20policy\n",
      "/narratives/Schismatic%20Ukrainian%20Church\n",
      "/narratives/Russia%20is%20not%20involved%20in%20MH17%20crash;%20Ukraine%20shot%20down%20the%20plane\n"
     ]
    }
   ],
   "source": [
    "result = '\\n'.join([item[0] for item in sub_headers])\n",
    "print(f'Here is sub-headers we are going to parse: \\n{result}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def get_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument(\"--disable-setuid-sandbox\")\n",
    "    return webdriver.Chrome(options=chrome_options)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3. Collect fake and true news"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "If we check narrative structure, we will see that it looks like:\n",
    "```text\n",
    "<h3>Fake news title</h3>\n",
    "<h4>True information</h4>\n",
    "```\n",
    "So we are going to store titles to our fake news dataset and text to our true one.\n",
    "As parser let's take `selenium`, because it helps us to parse dynamic pages. [4, 5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of titles: 6\n",
      "Number of titles: 8\n",
      "Number of titles: 5\n",
      "Number of titles: 8\n",
      "Number of titles: 3\n",
      "Number of titles: 3\n",
      "Number of titles: 11\n",
      "Number of titles: 4\n",
      "Number of titles: 4\n",
      "Number of titles: 4\n",
      "Number of titles: 9\n",
      "Number of titles: 3\n",
      "Number of titles: 1\n"
     ]
    }
   ],
   "source": [
    "driver = get_driver()\n",
    "\n",
    "for sub_header, label in sub_headers:\n",
    "    to_parse = url + sub_header # '<url>/narratives/<article's encoded title>\n",
    "    driver.get(to_parse)\n",
    "    titles = driver.find_elements(By.XPATH, '//*[@class=\"Narrative_fakeLink___YbTe\"]')\n",
    "    print(f'Number of titles: {len(titles)}')\n",
    "\n",
    "    for element in titles:\n",
    "        # Open dynamic link\n",
    "        element.click()\n",
    "\n",
    "        # Remove comment if we need to have time delay to do click\n",
    "        # time.sleep(1)\n",
    "\n",
    "        fake_news.add((element.text, label))\n",
    "\n",
    "        texts = element.find_elements(By.XPATH, '//*[@class=\"Narrative_Debunking__gRBl1\"]')\n",
    "        for text in texts:\n",
    "            true_news.add((text.text, label))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4. Import collected data to our dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "fake_news_result, fake_news_labels = zip(*fake_news)\n",
    "fakes = pd.DataFrame({\n",
    "    'text': fake_news_result,\n",
    "    'label': fake_news_labels,\n",
    "    'is_fake': 1\n",
    "})\n",
    "\n",
    "true_news_result, true_news_labels = zip(*true_news)\n",
    "trues = pd.DataFrame({\n",
    "    'text': true_news_result,\n",
    "    'label': true_news_labels,\n",
    "    'is_fake': 0\n",
    "})\n",
    "\n",
    "df = pd.concat([fakes, trues], ignore_index=True)\n",
    "df.to_csv('data/data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 5. Close and quit the driver to free up system resources"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "driver.close()\n",
    "driver.quit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "[1] - https://medium.com/swlh/scraping-a-dynamic-web-page-its-selenium-da161999c975\n",
    "[2] - https://rusdisinfo.voxukraine.org/\n",
    "[3] - https://pandas.pydata.org/\n",
    "[4] - https://blog.jovian.com/web-scraping-using-selenium-2a3ffa1f03f4\n",
    "[5] - https://medium.com/ymedialabs-innovation/web-scraping-using-beautiful-soup-and-selenium-for-dynamic-page-2f8ad15efe25"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}